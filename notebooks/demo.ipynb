{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66365728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 15:18:45.310629: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/rsenaferre/anaconda3_new/envs/ANITI_RuntimeMonitoringBenchmark/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-07-11 15:18:45.310646: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from dataset import Dataset\n",
    "from feature_extractor import FeatureExtractor\n",
    "from monitors_internals import MahalanobisMonitor, GaussianMixtureMonitor, OutsideTheBoxMonitor, MaxSoftmaxProbabilityMonitor,\\\n",
    "                    MaxLogitMonitor, EnergyMonitor, ReActMonitor\n",
    "from monitors_input import SHINE_monitor\n",
    "from evaluation import Evaluator\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2f6e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "model = \"resnet\"\n",
    "# model = \"densenet\"\n",
    "\n",
    "layers_ids = [2, 5, 8, 12, 15, 19, 22, 26, 29, 32]\n",
    "# layers_ids = [9, 19, 29, 40, 50, 60, 69, 79, 89, 98]\n",
    "\n",
    "id_dataset = \"cifar10\"\n",
    "\n",
    "ood_dataset = \"svhn\"\n",
    "\n",
    "additional_transform = None#\"pixelization\"\n",
    "adversarial_attack = None#\"fgsm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a85c76a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./Data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "dataset_train = Dataset(id_dataset, \"train\", model, batch_size=batch_size)\n",
    "dataset_test = Dataset(id_dataset, \"test\", model, batch_size=batch_size)\n",
    "dataset_ood = Dataset(ood_dataset, \"test\", model, additional_transform, adversarial_attack, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f3ae994",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor(model, id_dataset, layers_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8344734",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, logits_train, softmax_train, pred_train, lab_train = feature_extractor.get_features(dataset_train)\n",
    "features_test, logits_test, softmax_test, pred_test, lab_test = feature_extractor.get_features(dataset_test)\n",
    "features_ood, logits_ood, softmax_ood, pred_ood, lab_ood = feature_extractor.get_features(dataset_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c1620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f1c0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "ID:   0.9367\n",
      "OOD:  0\n"
     ]
    }
   ],
   "source": [
    "id_accuracy = accuracy_score(lab_test, pred_test)\n",
    "ood_accuracy = 0\n",
    "if id_dataset == ood_dataset:\n",
    "    ood_accuracy = accuracy_score(lab_ood, pred_ood)\n",
    "\n",
    "print(\"Accuracy\")\n",
    "print(\"ID:  \", id_accuracy)\n",
    "print(\"OOD: \", ood_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38f3c91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 5000/5000 [00:05<00:00, 833.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.shape(X), np.shape(y): (50000, 32, 32, 3) (50000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Converting training dataset (ID) from pytorch to numpy format\n",
    "num_samples = None\n",
    "\n",
    "X, y = [],[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(dataset_train.dataloader):\n",
    "        X.append(data[0].numpy())\n",
    "        y.append(data[1].numpy())\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X_train = np.reshape(X, (batch_size*X.shape[0], X.shape[3], X.shape[4], X.shape[2]))\n",
    "y_train = np.reshape(y, batch_size*y.shape[0])\n",
    "\n",
    "print('np.shape(X), np.shape(y):',np.shape(X_train), np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "143cbc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(id_X_train, id_y_train), (id_X_test, id_y_test) = utils.load_data(id_dataset, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a530df8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# verifying if labels are equal (torch2numpy, and data loaded manually)\n",
    "np.all(y_train == id_y_train)\n",
    "\n",
    "tensor_x = torch.Tensor(id_X_train) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(id_y_train)\n",
    "\n",
    "my_trainset = TensorDataset(tensor_x,tensor_y)\n",
    "my_trainset.name = id_dataset\n",
    "my_trainset.split=\"train\"\n",
    "my_trainset.network=model\n",
    "my_trainset.batch_size=batch_size\n",
    "features_train2, logits_train2, softmax_train2, pred_train2, lab_train2 = feature_extractor.get_features(my_trainset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be3af6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 50000/50000 [00:02<00:00, 22342.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.shape(X), np.shape(y): (50000, 32, 32, 3) (50000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataloader = DataLoader(my_trainset)\n",
    "# Converting training dataset (ID) from pytorch to numpy format\n",
    "num_samples = None\n",
    "\n",
    "X, y = [],[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(my_dataloader):\n",
    "        X.append(data[0].numpy())\n",
    "        y.append(data[1].numpy())\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X_train2 = np.reshape(X, (X.shape[0], X.shape[2], X.shape[3], X.shape[4]))\n",
    "y_train2 = np.reshape(y, y.shape[0])\n",
    "\n",
    "print('np.shape(X), np.shape(y):',np.shape(X_train2), np.shape(y_train2))\n",
    "\n",
    "np.all(lab_train == y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_oms = Evaluator(\"oms\", is_novelty=(id_dataset!=ood_dataset))\n",
    "\n",
    "eval_ood = Evaluator(\"ood\", is_novelty=(id_dataset!=ood_dataset))\n",
    "\n",
    "eval_oms.fit_ground_truth(lab_test, lab_ood, pred_test, pred_ood)\n",
    "eval_ood.fit_ground_truth(lab_test, lab_ood, pred_test, pred_ood)\n",
    "\n",
    "precision_star, recall_star, f1_star = eval_oms.get_metrics(eval_ood.y_true[:lab_test.shape[0]].astype(bool), \n",
    "                                             eval_ood.y_true[lab_test.shape[0]:].astype(bool))\n",
    "\n",
    "print(\"OMS results for perfect OOD detector\")\n",
    "print(\"Precision \", \" Recall \", \" F1\")\n",
    "print(\"{:.4f}\".format(precision_star), \"    \", \"{:.4f}\".format(recall_star), \" \", \"{:.4f}\".format(f1_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce29e79b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbd6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_layer_monitored = 3\n",
    "id_layer_monitored = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d870180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c15500",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_mahalanobis = MahalanobisMonitor(id_dataset, model, id_layer_monitored)\n",
    "monitor_mahalanobis.fit(features_train[id_layer_monitored], lab_train)\n",
    "\n",
    "scores_test_mahalanobis = monitor_mahalanobis.predict(features_test[id_layer_monitored], pred_test) \n",
    "scores_ood_mahalanobis = monitor_mahalanobis.predict(features_ood[id_layer_monitored], pred_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831a8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import get_auroc_ood, get_tnr_frac_tpr_ood\n",
    "\n",
    "monitor_mahalanobis = MahalanobisMonitor(id_dataset, model, id_layer_monitored, is_tied=True)\n",
    "monitor_mahalanobis.fit(features_train[id_layer_monitored], lab_train)\n",
    "\n",
    "scores_test_mahalanobis = monitor_mahalanobis.predict(features_test[id_layer_monitored], pred_test) \n",
    "scores_ood_mahalanobis = monitor_mahalanobis.predict(features_ood[id_layer_monitored], pred_ood)\n",
    "\n",
    "a = get_auroc_ood(scores_test_mahalanobis, scores_ood_mahalanobis)\n",
    "\n",
    "\n",
    "b = get_tnr_frac_tpr_ood(scores_test_mahalanobis, scores_ood_mahalanobis)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e7cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(scores_test_mahalanobis)\n",
    "plt.hist(scores_ood_mahalanobis, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece5400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import utils\n",
    "num_samples = None\n",
    "\n",
    "X, y = [],[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(dataset_train.dataloader):\n",
    "        X.append(data[0].numpy())\n",
    "        y.append(data[1].numpy())\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X = np.reshape(X, (batch_size*X.shape[0], X.shape[3], X.shape[4], X.shape[2]))\n",
    "y = np.reshape(y, batch_size*y.shape[0])\n",
    "\n",
    "print('np.shape(X), np.shape(y):',np.shape(X), np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building monitor with ID data\n",
    "monitor_shine = SHINE_monitor(id_dataset)\n",
    "monitor_shine.fit_by_class_parallel(X, y)\n",
    "\n",
    "print('number of monitors',len(monitor_shine.arr_density))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading test ID\n",
    "X, y = [],[]\n",
    "\n",
    "for data in tqdm(dataset_test.dataloader):\n",
    "    X.append(data[0].numpy())\n",
    "    y.append(data[1].numpy())\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(np.shape(X), np.shape(y), np.shape(pred_test))\n",
    "\n",
    "X = np.reshape(X, (batch_size*X.shape[0], X.shape[3], X.shape[4], X.shape[2]))\n",
    "y = np.reshape(y, batch_size*y.shape[0])\n",
    "\n",
    "print(np.shape(X), np.shape(y), np.shape(pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd07e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing ID\n",
    "arr_score_ID = {}\n",
    "arr_score_OOD = {}\n",
    "\n",
    "# looping through all monitored classes\n",
    "for monitored_class in range(10):\n",
    "    for x, pred, label in tqdm(zip(X, pred_test, y)):\n",
    "        pdf = monitor_shine.predict_parallel(np.array([x]), pred)\n",
    "\n",
    "        if monitored_class == label:\n",
    "            try:\n",
    "                arr_score_ID[monitored_class].append(pdf)\n",
    "            except:\n",
    "                arr_score_ID.update({monitored_class: [pdf]})\n",
    "        else:\n",
    "            try:\n",
    "                arr_score_OOD[monitored_class].append(pdf)\n",
    "            except:\n",
    "                arr_score_OOD.update({monitored_class: [pdf]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2281829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing OOD\n",
    "X_ood, y_ood = [],[]\n",
    "\n",
    "for monitored_class in range(10):\n",
    "    for data in tqdm(dataset_ood.dataloader):\n",
    "        X = data[0].numpy()\n",
    "        X_ood = np.reshape(X, (X.shape[0], X.shape[2], X.shape[3], X.shape[1]))\n",
    "        #y_ood = data[1].numpy()\n",
    "        #print(np.shape(X_ood), np.shape(y_ood))\n",
    "\n",
    "        for x, pred in zip(X_ood, pred_ood):\n",
    "            pdf = monitor_shine.predict_parallel(np.array([x]), pred)\n",
    "            \n",
    "            try:\n",
    "                arr_score_OOD[monitored_class].append(pdf)\n",
    "            except:\n",
    "                arr_score_OOD.update({monitored_class: [pdf]})\n",
    "            \n",
    "#arr_score_ID = np.array(arr_score_ID)\n",
    "#arr_score_OOD = np.array(arr_score_OOD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import get_auroc_ood, get_auroc_oms, get_tnr_frac_tpr_ood, get_tnr_frac_tpr_oms\n",
    "\n",
    "auroc, tnr_tpr = 0, 0\n",
    "for c in range(10): \n",
    "    # evaluating shine\n",
    "    print(c,np.shape(arr_score_ID[c]), np.shape(arr_score_OOD[c]))\n",
    "    a = get_auroc_ood(np.array(arr_score_ID[c]), np.array(arr_score_OOD[c]))\n",
    "    print('AUROC class',c,a)\n",
    "    auroc+=a\n",
    "\n",
    "    b = get_tnr_frac_tpr_ood(np.array(arr_score_ID[c]), np.array(arr_score_OOD[c]))\n",
    "    print('TNR/TPR class',c,b)\n",
    "    tnr_tpr+=b\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "print('avg', auroc/10,  tnr_tpr/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing score as a sum of class scores\n",
    "0.38845303664720343\n",
    "0.07098955132145052\n",
    "\n",
    "# testing score as based on a predicted class but together with the other classes \n",
    "0.42577552627535337\n",
    "0.3809157959434542\n",
    "\n",
    "# testing per monitor (oms)\n",
    "0.4292515820137766\n",
    "0.16028501781361335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2be4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluation\n",
    "#metrics\n",
    "\n",
    "print('SHINE roc curve')\n",
    "evaluation.plot_roc_curve(arr_score_ID, arr_score_OOD)\n",
    "\n",
    "print('SHINE other metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd190db",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_ood_shine, recall_ood_shine, f1_ood_shine = eval_ood.get_metrics(arr_is_ood_1.astype(bool), \n",
    "                                                                            arr_is_ood_2.astype(bool))\n",
    "#precision_oms_shine, recall_oms_shine, f1_oms_shine = eval_oms.get_metrics(scores_test_shine, \n",
    "#                                                                        scores_ood_shine)\n",
    "\n",
    "print(\"       \", \"Precision \", \" Recall \", \" F1\")\n",
    "print(\"OOD:   \", \"{:.4f}\".format(precision_ood_shine), \"    \", \"{:.4f}\".format(recall_ood_shine), \n",
    "      \" \", \"{:.4f}\".format(f1_ood_shine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5159a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "Precision   Recall   F1\n",
    "OOD:    0.6858      0.8316   0.7517\n",
    "\n",
    "#75\n",
    "Precision   Recall   F1\n",
    "OOD:    0.6855      0.8327   0.7520\n",
    "    \n",
    "#25\n",
    "Precision   Recall   F1\n",
    "OOD:    0.6858      0.8316   0.7517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca88fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d99e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping through all monitored classes\n",
    "monitored_classes = [i for i in range(10)]\n",
    "\n",
    "# scores for each monitored class\n",
    "arr_pred_test_per_class, arr_pred_ood_per_class = {}, {}\n",
    "\n",
    "for monitored_class in monitored_classes:\n",
    "    # we will then monitor the predicted class from the ML model\n",
    "    predicted_ML = [monitored_class]*len(ood_X_test)\n",
    "    arr_pred_test, arr_pred_ood = [], []\n",
    "    \n",
    "    for x, y_true, pred in zip(X_test, y_test, predicted_ML):\n",
    "        if y_true == monitored_class:\n",
    "            is_ood, shine_score = monitor_shine.predict(np.array([x]), pred)    \n",
    "            arr_pred_test.append(is_ood)\n",
    "            \n",
    "    arr_pred_test_per_class.update({monitored_class: np.array(arr_pred_test)})\n",
    "\n",
    "    for x, pred in zip(ood_X_test, predicted_ML):\n",
    "        is_ood, shine_score = monitor_shine.predict(np.array([x]), pred)    \n",
    "        arr_pred_ood.append(is_ood)\n",
    "        \n",
    "    arr_pred_ood_per_class.update({monitored_class: np.array(arr_pred_ood)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a3e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, matthews_corrcoef\n",
    "\n",
    "# converting the labels from test dataset to the format 1 and 0\n",
    "y_true = utils.convert_ood_labels(ood_y_test)\n",
    "# printing balanced accuracy\n",
    "acc = balanced_accuracy_score(y_true, arr_pred_ood)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_oms = Evaluator(\"oms\", is_novelty=(id_dataset!=ood_dataset))\n",
    "eval_ood = Evaluator(\"ood\", is_novelty=(id_dataset!=ood_dataset))\n",
    "\n",
    "for monitored_class in monitored_classes:\n",
    "    arr_pred_test, arr_pred_ood = arr_pred_test_per_class[monitored_class], arr_pred_ood_per_class[monitored_class]\n",
    "    #eval_oms.fit_ground_truth(y_test, ood_y_test, arr_pred_test, arr_pred_ood)\n",
    "    eval_ood.fit_ground_truth(y_test[y_test==monitored_class], ood_y_test, arr_pred_test, arr_pred_ood)\n",
    "\n",
    "    precision_ood_shine, recall_ood_shine, f1_ood_shine = eval_ood.get_metrics(arr_pred_test.astype(bool), \n",
    "                                                                            arr_pred_ood.astype(bool))\n",
    "    #precision_oms_shine, recall_oms_shine, f1_oms_shine = eval_oms.get_metrics(scores_test_shine, \n",
    "    #                                                                        scores_ood_shine)\n",
    "\n",
    "    print(\"Shine when monitoring the class\", monitored_class)\n",
    "    print(\"       \", \"Precision \", \" Recall \", \" F1\")\n",
    "    print(\"OOD:   \", \"{:.4f}\".format(precision_ood_shine), \"    \", \"{:.4f}\".format(recall_ood_shine), \n",
    "          \" \", \"{:.4f}\".format(f1_ood_shine))\n",
    "    #print(\"OMS:   \", \"{:.4f}\".format(precision_oms_shine), \"    \", \"{:.4f}\".format(recall_oms_shine), \n",
    "    #      \" \", \"{:.4f}\".format(f1_oms_shine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206677ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8bc05f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacee73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_oob = OutsideTheBoxMonitor(n_clusters=10)\n",
    "monitor_oob.fit(features_train[id_layer_monitored], lab_train)\n",
    "\n",
    "scores_oob_test = monitor_oob.predict(features_test[id_layer_monitored], pred_test)\n",
    "scores_oob_ood = monitor_oob.predict(features_ood[id_layer_monitored], pred_ood)\n",
    "\n",
    "precision_ood_oob, recall_ood_oob, f1_ood_oob = eval_ood.get_metrics(scores_oob_test.astype(bool), scores_oob_ood.astype(bool))\n",
    "precision_oms_oob, recall_oms_oob, f1_oms_oob = eval_oms.get_metrics(scores_oob_test.astype(bool), scores_oob_ood.astype(bool))\n",
    "\n",
    "print(\"Outside the box\")\n",
    "print(\"       \", \"Precision \", \" Recall \", \" F1\")\n",
    "print(\"OOD:   \", \"{:.4f}\".format(precision_ood_oob), \"    \", \"{:.4f}\".format(recall_ood_oob), \n",
    "      \" \", \"{:.4f}\".format(f1_ood_oob))\n",
    "print(\"OMS:   \", \"{:.4f}\".format(precision_oms_oob), \"    \", \"{:.4f}\".format(recall_oms_oob), \n",
    "      \" \", \"{:.4f}\".format(f1_oms_oob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e661a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a713a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_msp = MaxSoftmaxProbabilityMonitor()\n",
    "monitor_msp.fit()\n",
    "\n",
    "scores_test_msp = monitor_msp.predict(softmax_test) \n",
    "scores_ood_msp = monitor_msp.predict(softmax_ood) \n",
    "\n",
    "precision_ood_msp, recall_ood_msp, f1_ood_msp = eval_ood.get_metrics(scores_test_msp.astype(bool), scores_ood_msp.astype(bool))\n",
    "precision_oms_msp, recall_oms_msp, f1_oms_msp = eval_oms.get_metrics(scores_test_msp.astype(bool), scores_ood_msp.astype(bool))\n",
    "\n",
    "print(\"Max Softmax Probability\")\n",
    "print(\"       \", \"Precision \", \" Recall \", \" F1\")\n",
    "print(\"OOD:   \", \"{:.4f}\".format(precision_ood_msp), \"    \", \"{:.4f}\".format(recall_ood_msp), \n",
    "      \" \", \"{:.4f}\".format(f1_ood_msp))\n",
    "print(\"OMS:   \", \"{:.4f}\".format(precision_oms_msp), \"    \", \"{:.4f}\".format(recall_oms_msp), \n",
    "      \" \", \"{:.4f}\".format(f1_oms_msp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751c099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cad489",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_react = ReActMonitor(quantile_value=0.99, mode=\"msp\")\n",
    "monitor_react.fit(feature_extractor, features_train[-1])\n",
    "\n",
    "scores_test_react = monitor_react.predict(features_test[-1])\n",
    "scores_ood_react = monitor_react.predict(features_ood[-1])\n",
    "\n",
    "precision_ood_react_msp, recall_ood_react_msp, f1_ood_react_msp = eval_ood.get_metrics(scores_test_react, \n",
    "                                                                                       scores_ood_react)\n",
    "precision_oms_react_msp, recall_oms_react_msp, f1_oms_react_msp = eval_oms.get_metrics(scores_test_react, \n",
    "                                                                                       scores_ood_react)\n",
    "\n",
    "print(\"ReAct MSP\")\n",
    "print(\"       \", \"Precision \", \" Recall \", \" F1\")\n",
    "print(\"OOD:   \", \"{:.4f}\".format(precision_ood_react_msp), \"    \", \"{:.4f}\".format(recall_ood_react_msp), \n",
    "      \" \", \"{:.4f}\".format(f1_ood_react_msp))\n",
    "print(\"OMS:   \", \"{:.4f}\".format(precision_oms_react_msp), \"    \", \"{:.4f}\".format(recall_oms_react_msp), \n",
    "      \" \", \"{:.4f}\".format(f1_oms_react_msp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db35a8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43832d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_maxlogits = MaxLogitMonitor()\n",
    "monitor_maxlogits.fit()\n",
    "\n",
    "scores_test_maxlogits = monitor_maxlogits.predict(logits_test) \n",
    "scores_ood_maxlogits = monitor_maxlogits.predict(logits_ood) \n",
    "\n",
    "precision_ood, recall_ood, f1_ood = eval_ood.get_metrics(scores_test_maxlogits, scores_ood_maxlogits)\n",
    "precision_oms, recall_oms, f1_oms = eval_oms.get_metrics(scores_test_maxlogits, scores_ood_maxlogits)\n",
    "\n",
    "print(\"Max Logit\")\n",
    "print(\"       \", \"Precision \", \" Recall \", \" F1\")\n",
    "print(\"OOD:   \", \"{:.4f}\".format(precision_ood), \"    \", \"{:.4f}\".format(recall_ood), \" \", \"{:.4f}\".format(f1_ood))\n",
    "print(\"OMS:   \", \"{:.4f}\".format(precision_oms), \"    \", \"{:.4f}\".format(recall_oms), \" \", \"{:.4f}\".format(f1_oms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af7e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c35758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1\n",
    "\n",
    "monitor_energy = EnergyMonitor(temperature=T)\n",
    "monitor_energy.fit()\n",
    "\n",
    "scores_test_energy = monitor_energy.predict(logits_test)\n",
    "scores_ood_energy = monitor_energy.predict(logits_ood)\n",
    "\n",
    "precision_ood_energy, recall_ood_energy, f1_ood_energy = eval_ood.get_metrics(scores_test_energy, scores_ood_energy)\n",
    "precision_oms_energy, recall_oms_energy, f1_oms_energy = eval_oms.get_metrics(scores_test_energy, scores_ood_energy)\n",
    "\n",
    "print(\"Energy\")\n",
    "print(\"       \", \"Precision \", \" Recall \", \" F1\")\n",
    "print(\"OOD:   \", \"{:.4f}\".format(precision_ood_energy), \"    \", \"{:.4f}\".format(recall_ood_energy), \n",
    "      \" \", \"{:.4f}\".format(f1_ood_energy))\n",
    "print(\"OMS:   \", \"{:.4f}\".format(precision_oms_energy), \"    \", \"{:.4f}\".format(recall_oms_energy), \n",
    "      \" \", \"{:.4f}\".format(f1_oms_energy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547534ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f81a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_react = ReActMonitor(quantile_value=0.99)\n",
    "monitor_react.fit(feature_extractor, features_train[-1])\n",
    "\n",
    "scores_test_react = monitor_react.predict(features_test[-1])\n",
    "scores_ood_react = monitor_react.predict(features_ood[-1])\n",
    "\n",
    "precision_ood_react_ene, recall_ood_react_ene, f1_ood_react_ene = eval_ood.get_metrics(scores_test_react, \n",
    "                                                                                       scores_ood_react)\n",
    "precision_oms_react_ene, recall_oms_react_ene, f1_oms_react_ene = eval_oms.get_metrics(scores_test_react, \n",
    "                                                                                       scores_ood_react)\n",
    "\n",
    "print(\"ReAct Energy\")\n",
    "print(\"       \", \"Precision \", \" Recall \", \" F1\")\n",
    "print(\"OOD:   \", \"{:.4f}\".format(precision_ood_react_ene), \"    \", \"{:.4f}\".format(recall_ood_react_ene), \n",
    "      \" \", \"{:.4f}\".format(f1_ood_react_ene))\n",
    "print(\"OMS:   \", \"{:.4f}\".format(precision_oms_react_ene), \"    \", \"{:.4f}\".format(recall_oms_react_ene), \n",
    "      \" \", \"{:.4f}\".format(f1_oms_react_ene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c402d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
