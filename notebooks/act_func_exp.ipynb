{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4b6b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 11:11:03.032947: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/rsenaferre/anaconda3_new/envs/ANITI_RuntimeMonitoringBenchmark/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-07-17 11:11:03.032970: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from dataset import Dataset\n",
    "from feature_extractor import FeatureExtractor\n",
    "from monitors_internals import MahalanobisMonitor, GaussianMixtureMonitor, OutsideTheBoxMonitor, MaxSoftmaxProbabilityMonitor,\\\n",
    "                    MaxLogitMonitor, EnergyMonitor, ReActMonitor\n",
    "from monitors_input import SHINE_monitor, SHINE_monitor2\n",
    "from evaluation import Evaluator\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3126fba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "(73257, 32, 32, 3)\n",
      "(26032, 32, 32, 3)\n",
      "x_train shape: (73257, 32, 32, 3)\n",
      "73257 train samples\n",
      "26032 test samples\n"
     ]
    }
   ],
   "source": [
    "#model params\n",
    "batch_size = 10\n",
    "model = \"resnet\"\n",
    "\n",
    "#monitor params\n",
    "layer_relu_ids = [32]\n",
    "additional_transform = None\n",
    "adversarial_attack = None#\n",
    "\n",
    "#dataset params\n",
    "id_dataset = \"cifar10\"\n",
    "ood_dataset = \"svhn\"\n",
    "(id_X_train, id_y_train), (id_X_test, id_y_test) = utils.load_data(id_dataset, None)\n",
    "(ood_X_train, ood_y_train), (ood_X_test, ood_y_test) = utils.load_data(ood_dataset, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eebaee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "#loading data in pytorch way (for operations with ML models built with pytorch)\n",
    "tensor_x = torch.Tensor(id_X_train) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(id_y_train)\n",
    "\n",
    "my_trainset = TensorDataset(tensor_x,tensor_y)\n",
    "my_trainset.name = id_dataset\n",
    "my_trainset.split=\"train\"\n",
    "my_trainset.network=model\n",
    "my_trainset.additional_transform=None\n",
    "my_trainset.adversarial_attack=None\n",
    "my_trainset.batch_size=batch_size\n",
    "\n",
    "#test\n",
    "tensor_x = torch.Tensor(id_X_test) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(id_y_test)\n",
    "my_testset = TensorDataset(tensor_x,tensor_y)\n",
    "my_testset.name = id_dataset\n",
    "my_testset.split=\"test\"\n",
    "my_testset.network=model\n",
    "my_testset.additional_transform=None\n",
    "my_testset.adversarial_attack=None\n",
    "my_testset.batch_size=batch_size\n",
    "\n",
    "#ood\n",
    "tensor_x = torch.Tensor(id_X_test) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(id_y_test)\n",
    "my_dataset_ood = TensorDataset(tensor_x,tensor_y)\n",
    "my_dataset_ood.name = ood_dataset\n",
    "my_dataset_ood.split=\"test\"\n",
    "my_dataset_ood.network=model\n",
    "my_dataset_ood.additional_transform=None\n",
    "my_dataset_ood.adversarial_attack=None\n",
    "my_dataset_ood.batch_size=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712193e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting and storing features from ID and OOD data using an ML model trained on ID data\n",
    "feature_extractor = FeatureExtractor(model, id_dataset, layer_relu_ids)\n",
    "\n",
    "features_train, logits_train, softmax_train, pred_train, lab_train = feature_extractor.get_features(my_trainset)\n",
    "features_test, logits_test, softmax_test, pred_test, lab_test = feature_extractor.get_features(my_testset)\n",
    "features_ood, logits_ood, softmax_ood, pred_ood, lab_ood = feature_extractor.get_features(my_dataset_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d21f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "ID:   0.9367\n",
      "OOD:  0\n"
     ]
    }
   ],
   "source": [
    "#accuracy of the ML model \n",
    "id_accuracy = accuracy_score(lab_test, pred_test)\n",
    "ood_accuracy = 0\n",
    "if id_dataset == ood_dataset:\n",
    "    ood_accuracy = accuracy_score(lab_ood, pred_ood)\n",
    "\n",
    "print(\"Accuracy\")\n",
    "print(\"ID:  \", id_accuracy)\n",
    "print(\"OOD: \", ood_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b2db5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_layer_monitored = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe6e3439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def similarity_act_func(features, id_y_train, pred_train):\n",
    "    for cls in range(10):\n",
    "        scores_1 = []\n",
    "        scores_2 = []\n",
    "        \n",
    "\n",
    "        #all labels from class c\n",
    "        ind_y_c = np.where(id_y_train == cls)[0]\n",
    "        #print('len all labels class',c, np.shape(ind_y_c), ind_y_c)\n",
    "\n",
    "        #all pred as c\n",
    "        ind_ML_c = np.where(pred_train==cls)[0]\n",
    "        #print('len pred class',c, np.shape(ind_ML_c), ind_ML_c)\n",
    "\n",
    "        #features from correct pred\n",
    "        ind = set(ind_y_c).intersection(ind_ML_c)\n",
    "        f_c_correct = features[list(ind)]\n",
    "\n",
    "        #features from incorrect pred\n",
    "        ind = set(ind_y_c).symmetric_difference(ind_ML_c)\n",
    "        f_c_incorrect = features[list(ind)]\n",
    "\n",
    "        # how similar are act functions between themselves?\n",
    "        for i in f_c_incorrect:\n",
    "            for c in f_c_correct:\n",
    "                cosine_similarity = 1 - spatial.distance.cosine(c, i)\n",
    "                scores_1.append(cosine_similarity)\n",
    "\n",
    "        for i in range(1,len(f_c_correct)):\n",
    "            cosine_similarity = 1 - spatial.distance.cosine(f_c_correct[i-1], f_c_correct[i])\n",
    "            scores_2.append(cosine_similarity)\n",
    "\n",
    "        print('sim correct/incorrect pred', np.sum(scores_1)/len(scores_1))\n",
    "        print('sim between pairs de correct pred', np.sum(scores_2)/len(scores_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58199ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis for training set\n",
      "sim correct/incorrect pred 0.7043612875817387\n",
      "sim between pairs de correct pred 0.949986131458183\n",
      "sim correct/incorrect pred 0.7919576565339418\n",
      "sim between pairs de correct pred 0.9692505286387355\n",
      "sim correct/incorrect pred 0.6996965759342939\n",
      "sim between pairs de correct pred 0.9494288562652652\n",
      "sim correct/incorrect pred 0.7209884673180724\n",
      "sim between pairs de correct pred 0.9340847544688423\n",
      "sim correct/incorrect pred 0.7435958852703616\n",
      "sim between pairs de correct pred 0.9624540209054374\n",
      "sim correct/incorrect pred 0.7924619650914049\n",
      "sim between pairs de correct pred 0.9582755219898194\n",
      "sim correct/incorrect pred 0.6745569607420177\n",
      "sim between pairs de correct pred 0.9417321706201949\n",
      "sim correct/incorrect pred 0.748138423073139\n",
      "sim between pairs de correct pred 0.9641624241261869\n",
      "sim correct/incorrect pred 0.6934741679642413\n",
      "sim between pairs de correct pred 0.9695176842764563\n",
      "sim correct/incorrect pred 0.7831512270012185\n",
      "sim between pairs de correct pred 0.9695327399420042\n",
      "analysis for test set\n",
      "analysis for OOD test set\n"
     ]
    }
   ],
   "source": [
    "#training set\n",
    "print('analysis for training set')\n",
    "features = features_train[id_layer_monitored]\n",
    "similarity_act_func(features, id_y_train, pred_train)\n",
    "\n",
    "#test set\n",
    "print('analysis for test set')\n",
    "features = features_test[id_layer_monitored]\n",
    "#similarity_act_func(features, id_y_test, pred_test)\n",
    "\n",
    "#OOD set\n",
    "print('analysis for OOD test set')\n",
    "features = features_ood[id_layer_monitored]\n",
    "#similarity_act_func(features, ood_y_test, pred_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34db3fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of monitors 10\n",
      "number of monitors 10\n"
     ]
    }
   ],
   "source": [
    "#building SHINE monitor with ID data\n",
    "monitor_shine = SHINE_monitor(id_dataset)\n",
    "#monitor_shine.fit_by_class_parallel(X_train, y_train)\n",
    "monitor_shine.fit_by_class(id_X_train, id_y_train, pred_train)\n",
    "\n",
    "print('number of monitors',len(monitor_shine.arr_density))\n",
    "\n",
    "\n",
    "#building SHINE 2 monitor with ID data\n",
    "monitor_shine2 = SHINE_monitor2(id_dataset)\n",
    "#monitor_shine.fit_by_class_parallel(X_train, y_train)\n",
    "monitor_shine2.fit_by_class(id_X_train, id_y_train, pred_train)\n",
    "\n",
    "print('number of monitors',len(monitor_shine2.arr_density))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f89ca152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same analysis but for SHINE scores\n",
    "\n",
    "def shine_perf_analysis(id_X_train, id_y_training, pred_train, threshold_id):\n",
    "    for c in range(10):\n",
    "        scores_1 = []\n",
    "        scores_2 = []\n",
    "        #threshold_c = monitor_shine.scores_per_class[c]\n",
    "\n",
    "        #all labels from class c\n",
    "        ind_y_c = np.where(id_y_training == c)[0]\n",
    "        #print('len all labels class',c, np.shape(ind_y_c), ind_y_c)\n",
    "\n",
    "        #all pred as c\n",
    "        ind_ML_c = np.where(pred_train==c)[0]\n",
    "        #print('len pred class',c, np.shape(ind_ML_c), ind_ML_c)\n",
    "\n",
    "        #images from correct pred\n",
    "        ind = set(ind_y_c).intersection(ind_ML_c)\n",
    "        X_c_correct = id_X_train[list(ind)]\n",
    "\n",
    "        #scores from correct\n",
    "        for x, pred in zip(X_c_correct, pred_train[list(ind)]):\n",
    "            monitor_pred, pdf = monitor_shine.predict(np.array([x]), pred, threshold_id)\n",
    "            scores_1.append(monitor_pred)\n",
    "\n",
    "        #images from incorrect pred\n",
    "        ind = set(ind_y_c).symmetric_difference(ind_ML_c)\n",
    "        X_c_incorrect = id_X_train[list(ind)]\n",
    "\n",
    "        #scores from incorrect\n",
    "        for x, pred in zip(X_c_incorrect, pred_train[list(ind)]):\n",
    "            monitor_pred, pdf = monitor_shine.predict(np.array([x]), pred, threshold_id)\n",
    "            scores_2.append(monitor_pred)\n",
    "\n",
    "        print('avg score for correct pred', 1-(np.sum(scores_1)/len(scores_1)))\n",
    "        print('avg score for incorrect pred', 1-(np.sum(scores_2)/len(scores_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70099f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set analysis\n",
      "testing set analysis\n",
      "OOD test set analysis\n"
     ]
    }
   ],
   "source": [
    "threshold_id = 0.9\n",
    "\n",
    "#training set\n",
    "print('training set analysis')\n",
    "#shine_perf_analysis(id_X_train, id_y_train, pred_train, threshold_id)\n",
    "\n",
    "#testing set\n",
    "print('testing set analysis')\n",
    "#shine_perf_analysis(id_X_test, id_y_test, pred_test, threshold_id)\n",
    "\n",
    "#testing set\n",
    "print('OOD test set analysis')\n",
    "#shine_perf_analysis(ood_X_test, ood_y_test, pred_ood, threshold_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "951cf4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hybrid version of SHINE\n",
    "arr_ood_threshold = {}\n",
    "#arr_ood_threshold.update({0: 0.7043, 1: 0.7919, 2: 0.6996, 3: 0.7209, 4:0.7435, 5:0.7924, 6:0.6745, 7:0.7481,\n",
    "#                         8:0.6934, 9:0.7831})\n",
    "arr_ood_threshold.update({0: 0.7, 1: 0.7, 2: 0.6, 3: 0.7, 4:0.7, 5:0.7, 6:0.6, 7:0.7, 8:0.6, 9:0.7})\n",
    "\n",
    "arr_id_threshold = {}\n",
    "arr_id_threshold.update({0:0.9499, 1:0.9692, 2:0.9494, 3:0.9340, 4:0.9624, 5:0.9582, 6:0.9417, 7:0.9641,\n",
    "                        8:0.9695, 9:0.9695})\n",
    "\n",
    "#arr_id_threshold.update({0:0.94, 1:0.96, 2:0.94, 3:0.93, 4:0.96, 5:0.95, 6:0.94, 7:0.96, 8:0.96, 9:0.96})\n",
    "\n",
    "def run_new_SHINE(monitor_shine, X, pred, incoming_feature, threshold_shine):\n",
    "    features = features_train[id_layer_monitored]\n",
    "    #all labels from class c\n",
    "    ind_y_c = np.where(id_y_train == pred)[0]\n",
    "    #print('len all labels class',c, np.shape(ind_y_c), ind_y_c)\n",
    "\n",
    "    #all pred as c\n",
    "    ind_ML_c = np.where(pred_train==pred)[0]\n",
    "    #print('len pred class',c, np.shape(ind_ML_c), ind_ML_c)\n",
    "\n",
    "    #features from correct pred\n",
    "    ind = set(ind_y_c).intersection(ind_ML_c)\n",
    "    f_c_correct = features[list(ind)]\n",
    "\n",
    "    scores = []\n",
    "    for c in f_c_correct:\n",
    "        cosine_similarity = 1 - spatial.distance.cosine(c, incoming_feature)\n",
    "        scores.append(cosine_similarity)\n",
    "        \n",
    "    avg_sim = np.sum(scores)/len(scores)\n",
    "    \n",
    "    if avg_sim >= arr_id_threshold[pred]:\n",
    "        return False #it is ID and correct pred\n",
    "    elif avg_sim < arr_ood_threshold[pred]:\n",
    "        return True #it is OOD\n",
    "    else:\n",
    "        #it is not OOD but we do not know if the prediction is correct\n",
    "        monitor_pred, pdf = monitor_shine.predict(X, pred, threshold_shine)\n",
    "        return monitor_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ac6843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## experimenting new thresholds for shine\n",
    "def run_SHINE_2(monitor_shine, X, pred, incoming_feature, threshold_SMimg, threshold_SMout):\n",
    "    features = features_train[id_layer_monitored]\n",
    "    #all labels from class c\n",
    "    ind_y_c = np.where(id_y_train == pred)[0]\n",
    "    #print('len all labels class',c, np.shape(ind_y_c), ind_y_c)\n",
    "\n",
    "    #all pred as c\n",
    "    ind_ML_c = np.where(pred_train==pred)[0]\n",
    "    #print('len pred class',c, np.shape(ind_ML_c), ind_ML_c)\n",
    "\n",
    "    #features from correct pred\n",
    "    ind = set(ind_y_c).intersection(ind_ML_c)\n",
    "    f_c_correct = features[list(ind)]\n",
    "\n",
    "    scores = []\n",
    "    for c in f_c_correct:\n",
    "        cosine_similarity = 1 - spatial.distance.cosine(c, incoming_feature)\n",
    "        scores.append(cosine_similarity)\n",
    "        \n",
    "    len_scores = len(scores)\n",
    "    sorted_scores = sorted(scores)\n",
    "    ind_threshold = int(len_scores*threshold_SMout)\n",
    "    min_threshold_sim = sorted_scores[-ind_threshold]\n",
    "    max_threshold_sim = sorted_scores[ind_threshold]\n",
    "    #avg_sim = np.sum(scores)/len(scores)\n",
    "    \n",
    "    if max_threshold_sim >= arr_id_threshold[pred]:\n",
    "        return False #it is ID and correct pred\n",
    "    elif min_threshold_sim <= arr_ood_threshold[pred]:\n",
    "        return True #it is OOD\n",
    "    else:\n",
    "        #it is not OOD but we do not know if the prediction is correct\n",
    "        monitor_pred, pdf = monitor_shine.predict(X, pred, threshold_SMimg)\n",
    "        return monitor_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eff15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building oob monitor\n",
    "monitor_oob = OutsideTheBoxMonitor(n_clusters=3)\n",
    "monitor_oob.fit(features_train[id_layer_monitored], lab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ad23c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [30:45,  5.42it/s]\n"
     ]
    }
   ],
   "source": [
    "#testing SHINE  and OOB monitors monitors on ID testset\n",
    "m_true = []\n",
    "m_shine = []\n",
    "m_shine_2 = []\n",
    "m_oob = []\n",
    "\n",
    "threshold_SMimg = 0.1\n",
    "threshold_SMout = 0.9\n",
    "\n",
    "for x, pred, feature, label in tqdm(zip(id_X_test, pred_test, \n",
    "                                        features_test[id_layer_monitored], id_y_test.flatten())):\n",
    "    #SHINE\n",
    "    #monitor_shine_pred = run_new_SHINE(monitor_shine2, np.array([x]), pred, feature, threshold_id)\n",
    "    #m_shine.append(monitor_shine_pred)\n",
    "    \n",
    "    # SHINE relaxed thresholds\n",
    "    monitor_shine_pred_2 = run_SHINE_2(monitor_shine2, np.array([x]), pred, feature,\n",
    "                                       threshold_SMimg, threshold_SMout)\n",
    "    m_shine_2.append(monitor_shine_pred_2)\n",
    "    \n",
    "    #oob\n",
    "    #out_box = monitor_oob.predict([feature], [pred])\n",
    "    #m_oob.append(out_box)\n",
    "    \n",
    "    if pred == label: #monitor does not need to activate\n",
    "        m_true.append(0)\n",
    "    else: #monitor should activate\n",
    "        m_true.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501264ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2e6be7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26032it [1:18:05,  5.56it/s]\n"
     ]
    }
   ],
   "source": [
    "for x, feature, pred in tqdm(zip(ood_X_test, features_ood[id_layer_monitored], pred_ood)):\n",
    "    #SHINE\n",
    "    #monitor_shine_pred = run_new_SHINE(monitor_shine2, np.array([x]), pred, feature, threshold_id)\n",
    "    #m_shine.append(monitor_shine_pred)\n",
    "    \n",
    "    # SHINE relaxed thresholds\n",
    "    monitor_shine_pred_2 = run_SHINE_2(monitor_shine2, np.array([x]), pred, feature,\n",
    "                                       threshold_SMimg, threshold_SMout)\n",
    "    m_shine_2.append(monitor_shine_pred_2)\n",
    "    \n",
    "    #oob\n",
    "    #out_box = monitor_oob.predict([feature], [pred])\n",
    "    #m_oob.append(out_box)\n",
    "    \n",
    "    m_true.append(1) #monitor should always react to novel classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5354b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6cdfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training sgd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21992a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## doing the same tests but with shine using a shallow ml model\n",
    "#ID\n",
    "for x, pred, feature, label in tqdm(zip(id_X_test, pred_test, \n",
    "                                        features_test[id_layer_monitored], id_y_test.flatten())):\n",
    "    #SHINE\n",
    "    #monitor_shine_pred = run_new_SHINE(monitor_shine2, np.array([x]), pred, feature, threshold_id)\n",
    "    #m_shine.append(monitor_shine_pred)\n",
    "    \n",
    "    # SHINE relaxed thresholds\n",
    "    monitor_shine_pred_2 = run_SHINE_2(monitor_shine2, np.array([x]), pred, feature,\n",
    "                                       threshold_SMimg, threshold_SMout)\n",
    "    m_shine_2.append(monitor_shine_pred_2)\n",
    "    \n",
    "    #oob\n",
    "    #out_box = monitor_oob.predict([feature], [pred])\n",
    "    #m_oob.append(out_box)\n",
    "    \n",
    "    if pred == label: #monitor does not need to activate\n",
    "        m_true.append(0)\n",
    "    else: #monitor should activate\n",
    "        m_true.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "## doing the same tests but with shine using a shallow ml model\n",
    "#OOD\n",
    "for x, feature, pred in tqdm(zip(ood_X_test, features_ood[id_layer_monitored], pred_ood)):\n",
    "    \n",
    "    # SHINE with shallow ml models\n",
    "    pred_shine_sgd = run_SHINE_sgd(monitor_shine_sgd, np.array([x]), pred, feature,\n",
    "                                       threshold_SMimg, threshold_SMout)\n",
    "    m_shine_sgd.append(pred_shine_sgd)\n",
    "    \n",
    "    m_true.append(1) #monitor should always react to novel classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1785ac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SHINE new 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.62      0.86      0.72      9367\n",
      "        True       0.94      0.82      0.87     26665\n",
      "\n",
      "    accuracy                           0.83     36032\n",
      "   macro avg       0.78      0.84      0.80     36032\n",
      "weighted avg       0.86      0.83      0.83     36032\n",
      "\n",
      "0.6154115851660942\n",
      "0.8363072794804025\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef as mcc\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "m_true = np.array(m_true).astype(bool)\n",
    "#m_shine = np.array(m_shine).astype(bool)\n",
    "#m_shine_2 = np.array(m_shine_2).astype(bool)\n",
    "m_shine_sgd = np.array(m_shine_sgd).astype(bool)\n",
    "#m_oob = np.array(m_oob).flatten().astype(bool)\n",
    "\n",
    "#evaluating new SHINE \n",
    "#print('\\nSHINE new')\n",
    "#print(classification_report(m_true, m_shine))\n",
    "#print(mcc(m_true, m_shine))\n",
    "#print(balanced_accuracy_score(m_true, m_shine))\n",
    "\n",
    "#evaluating new SHINE 2\n",
    "print('\\nSHINE new 2')\n",
    "print(classification_report(m_true, m_shine_2))\n",
    "print(mcc(m_true, m_shine_2))\n",
    "print(balanced_accuracy_score(m_true, m_shine_2))\n",
    "\n",
    "#evaluating OOB\n",
    "#print('\\nOOB')\n",
    "#print(classification_report(m_true, m_oob))\n",
    "#print(mcc(m_true, m_oob))\n",
    "#print(balanced_accuracy_score(m_true, m_oob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6b7fa56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSHINE new\\n              precision    recall  f1-score   support\\n\\n       False       0.50      0.94      0.65      9367\\n        True       0.97      0.67      0.79     26665\\n\\n    accuracy                           0.74     36032\\n   macro avg       0.73      0.80      0.72     36032\\nweighted avg       0.85      0.74      0.75     36032\\n\\n0.5306945207656718\\n0.8024463443131951\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "id_threshold = int(len_scores*threshold_shine)\n",
    "min_threshold_sim = sorted_scores[-id_threshold]\n",
    "max_threshold_sim = sorted_scores[id_threshold]\n",
    "\n",
    "SHINE new 2\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       False       0.62      0.86      0.72      9367\n",
    "        True       0.94      0.82      0.87     26665\n",
    "\n",
    "    accuracy                           0.83     36032\n",
    "   macro avg       0.78      0.84      0.80     36032\n",
    "weighted avg       0.86      0.83      0.83     36032\n",
    "\n",
    "0.6156203195994662\n",
    "0.8364962945766474\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
