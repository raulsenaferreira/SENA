{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4b6b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 07:14:23.103226: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/rsenaferre/anaconda3_new/envs/ANITI_RuntimeMonitoringBenchmark/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2022-07-26 07:14:23.103245: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from dataset import Dataset\n",
    "from feature_extractor import FeatureExtractor\n",
    "from monitors_internals import MahalanobisMonitor, GaussianMixtureMonitor, OutsideTheBoxMonitor, MaxSoftmaxProbabilityMonitor,\\\n",
    "                    MaxLogitMonitor, EnergyMonitor, ReActMonitor\n",
    "from monitors_input import SHINE_monitor, SHINE_monitor2\n",
    "from evaluation import Evaluator\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3126fba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = utils.load_data('mnist', num_samples=None)\n",
    "(ood_X_train, ood_y_train), (ood_X_test, ood_y_test) = utils.load_data('fashion_mnist', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06acb6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9014166666666666\n"
     ]
    }
   ],
   "source": [
    "filename = 'models/sgd_mnist.sav'\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#training ID dataset\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "nsamples, nx, ny, ndim = x_train.shape\n",
    "train_dataset = x_train.reshape((nsamples,nx*ny*ndim))\n",
    "x_train_scaled = scaler.fit_transform(train_dataset.astype(np.float64))\n",
    "\n",
    "pred_train = loaded_model.predict(x_train_scaled)\n",
    "\n",
    "print(accuracy_score(pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eebaee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing ID...\n",
      "0.8971\n"
     ]
    }
   ],
   "source": [
    "#testing over ID test\n",
    "print('testing ID...')\n",
    "nsamples, nx, ny, ndim = x_test.shape\n",
    "test_dataset = x_test.reshape((nsamples,nx*ny*ndim))\n",
    "\n",
    "x_test_scaled = scaler.fit_transform(test_dataset.astype(np.float64))\n",
    "\n",
    "pred_test = loaded_model.predict(x_test_scaled)\n",
    "\n",
    "print(accuracy_score(pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "712193e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing OOD...\n",
      "0.1474\n"
     ]
    }
   ],
   "source": [
    "#testing over OOD test\n",
    "filename = 'models/sgd_mnist.sav'\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "print('testing OOD...')\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "nsamples, nx, ny, ndim = ood_X_test.shape\n",
    "ood_test_dataset = ood_X_test.reshape((nsamples,nx*ny*ndim))\n",
    "\n",
    "ood_x_test_scaled = scaler.fit_transform(ood_test_dataset.astype(np.float64))\n",
    "\n",
    "pred_ood = loaded_model.predict(ood_x_test_scaled)\n",
    "\n",
    "print(accuracy_score(pred_ood, ood_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d21f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# internal values\n",
    "print('features from the image', loaded_model.decision_function([x_test_scaled[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2db5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe6e3439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import pickle\n",
    "filename_c = 'Features/SGD/mnist_{}_correct.sav'\n",
    "filename_i = 'Features/SGD/mnist_{}_incorrect.sav'\n",
    "\n",
    "def similarity_act_func(features, id_y_train, pred_train):\n",
    "    for cls in range(10):\n",
    "        scores_1 = []\n",
    "        scores_2 = []\n",
    "        \n",
    "        #all labels from class c\n",
    "        ind_y_c = np.where(id_y_train == cls)[0]\n",
    "        #print('len all labels class',c, np.shape(ind_y_c), ind_y_c)\n",
    "\n",
    "        #all pred as c\n",
    "        ind_ML_c = np.where(pred_train==cls)[0]\n",
    "        #print('len pred class',c, np.shape(ind_ML_c), ind_ML_c)\n",
    "\n",
    "        #features from correct pred\n",
    "        ind = set(ind_y_c).intersection(ind_ML_c)\n",
    "        f_c_correct = features[list(ind)]\n",
    "\n",
    "        #features from incorrect pred\n",
    "        ind = set(ind_y_c).symmetric_difference(ind_ML_c)\n",
    "        f_c_incorrect = features[list(ind)]\n",
    "\n",
    "        # how similar are act functions between themselves?\n",
    "        for i in f_c_incorrect:\n",
    "            for c in f_c_correct:\n",
    "                cosine_similarity = 1 - spatial.distance.cosine(c, i)\n",
    "                scores_1.append(cosine_similarity)\n",
    "\n",
    "        for i in range(1,len(f_c_correct)):\n",
    "            cosine_similarity = 1 - spatial.distance.cosine(f_c_correct[i-1], f_c_correct[i])\n",
    "            scores_2.append(cosine_similarity)\n",
    "            \n",
    "        pickle.dump(scores_1, open(filename_i.format(cls), 'wb'))\n",
    "        pickle.dump(scores_2, open(filename_c.format(cls), 'wb'))\n",
    "        print('sim correct/incorrect pred', np.sum(scores_1)/len(scores_1))\n",
    "        print('sim between pairs de correct pred', np.sum(scores_2)/len(scores_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf647f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim correct/incorrect pred 0.8802507817600072\n",
      "sim between pairs de correct pred 0.9443786893853416\n",
      "sim correct/incorrect pred 0.8761003266581424\n",
      "sim between pairs de correct pred 0.9494050133214748\n",
      "sim correct/incorrect pred 0.7754160302346598\n",
      "sim between pairs de correct pred 0.8667444661828739\n",
      "sim correct/incorrect pred 0.8236260199548552\n",
      "sim between pairs de correct pred 0.8742108164545691\n",
      "sim correct/incorrect pred 0.8171518240722214\n",
      "sim between pairs de correct pred 0.8853001784424309\n",
      "sim correct/incorrect pred 0.8036872757125182\n",
      "sim between pairs de correct pred 0.8479743085284701\n",
      "sim correct/incorrect pred 0.8452646012618847\n",
      "sim between pairs de correct pred 0.9115823137034317\n",
      "sim correct/incorrect pred 0.8422245282543268\n",
      "sim between pairs de correct pred 0.912887384930604\n",
      "sim correct/incorrect pred 0.7994836146220285\n",
      "sim between pairs de correct pred 0.8690420363361099\n",
      "sim correct/incorrect pred 0.8606543307953501\n",
      "sim between pairs de correct pred 0.9142232165528579\n"
     ]
    }
   ],
   "source": [
    "#calculating thresholds\n",
    "#training set\n",
    "#print('analysis for training set')\n",
    "features_train = loaded_model.decision_function(x_train_scaled)\n",
    "similarity_act_func(features_train, y_train, pred_train)\n",
    "\n",
    "\n",
    "#test set\n",
    "#print('analysis for test set')\n",
    "features_test = loaded_model.decision_function(x_test_scaled)\n",
    "#similarity_act_func(features_test, y_test, pred_test)\n",
    "\n",
    "#OOD set\n",
    "#print('analysis for OOD test set')\n",
    "features_ood = loaded_model.decision_function(ood_x_test_scaled)\n",
    "#similarity_act_func(features_ood, ood_y_test, pred_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34db3fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bandwidth': 1.0}\n",
      "{'bandwidth': 1.0}\n",
      "{'bandwidth': 1.0}\n",
      "{'bandwidth': 1.0}\n",
      "{'bandwidth': 1.0}\n",
      "{'bandwidth': 1.0}\n",
      "{'bandwidth': 1.0}\n",
      "{'bandwidth': 1.0}\n",
      "{'bandwidth': 1.0}\n",
      "{'bandwidth': 1.0}\n",
      "number of monitors 10\n"
     ]
    }
   ],
   "source": [
    "#building SHINE monitor with ID data\n",
    "id_dataset = 'mnist'\n",
    "monitor_shine = SHINE_monitor(id_dataset)\n",
    "#monitor_shine.fit_by_class_parallel(X_train, y_train)\n",
    "monitor_shine.fit_by_class(x_train, y_train, pred_train)\n",
    "\n",
    "print('number of monitors',len(monitor_shine.arr_density))\n",
    "\n",
    "\n",
    "#building SHINE 2 monitor with ID data\n",
    "#monitor_shine2 = SHINE_monitor2(id_dataset)\n",
    "#monitor_shine.fit_by_class_parallel(X_train, y_train)\n",
    "#monitor_shine2.fit_by_class(id_X_train, id_y_train, pred_train)\n",
    "\n",
    "#print('number of monitors',len(monitor_shine2.arr_density))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89ca152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same analysis but for SHINE scores\n",
    "\n",
    "def shine_perf_analysis(id_X_train, id_y_training, pred_train, threshold_id):\n",
    "    for c in range(10):\n",
    "        scores_1 = []\n",
    "        scores_2 = []\n",
    "        #threshold_c = monitor_shine.scores_per_class[c]\n",
    "\n",
    "        #all labels from class c\n",
    "        ind_y_c = np.where(id_y_training == c)[0]\n",
    "        #print('len all labels class',c, np.shape(ind_y_c), ind_y_c)\n",
    "\n",
    "        #all pred as c\n",
    "        ind_ML_c = np.where(pred_train==c)[0]\n",
    "        #print('len pred class',c, np.shape(ind_ML_c), ind_ML_c)\n",
    "\n",
    "        #images from correct pred\n",
    "        ind = set(ind_y_c).intersection(ind_ML_c)\n",
    "        X_c_correct = id_X_train[list(ind)]\n",
    "\n",
    "        #scores from correct\n",
    "        for x, pred in zip(X_c_correct, pred_train[list(ind)]):\n",
    "            monitor_pred, pdf = monitor_shine.predict(np.array([x]), pred, threshold_id)\n",
    "            scores_1.append(monitor_pred)\n",
    "\n",
    "        #images from incorrect pred\n",
    "        ind = set(ind_y_c).symmetric_difference(ind_ML_c)\n",
    "        X_c_incorrect = id_X_train[list(ind)]\n",
    "\n",
    "        #scores from incorrect\n",
    "        for x, pred in zip(X_c_incorrect, pred_train[list(ind)]):\n",
    "            monitor_pred, pdf = monitor_shine.predict(np.array([x]), pred, threshold_id)\n",
    "            scores_2.append(monitor_pred)\n",
    "\n",
    "        print('avg score for correct pred', 1-(np.sum(scores_1)/len(scores_1)))\n",
    "        print('avg score for incorrect pred', 1-(np.sum(scores_2)/len(scores_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70099f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set analysis\n",
      "testing set analysis\n",
      "OOD test set analysis\n"
     ]
    }
   ],
   "source": [
    "threshold_id = 0.9\n",
    "\n",
    "#training set\n",
    "print('training set analysis')\n",
    "#shine_perf_analysis(id_X_train, id_y_train, pred_train, threshold_id)\n",
    "\n",
    "#testing set\n",
    "print('testing set analysis')\n",
    "#shine_perf_analysis(id_X_test, id_y_test, pred_test, threshold_id)\n",
    "\n",
    "#testing set\n",
    "print('OOD test set analysis')\n",
    "#shine_perf_analysis(ood_X_test, ood_y_test, pred_ood, threshold_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "951cf4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hybrid version of SHINE\n",
    "arr_ood_threshold = {}\n",
    "arr_ood_threshold.update({0: 0.8802, 1: 0.8761, 2: 0.7754, 3: 0.8236, 4: 0.8171,\n",
    "                          5:0.8036, 6:0.8452, 7:0.8422, 8:7994., 9:0.8606})\n",
    "\n",
    "arr_id_threshold = {}\n",
    "arr_id_threshold.update({0:0.9443, 1:0.9494, 2:0.8667, 3:0.8742, 4:0.8853,\n",
    "                         5:0.8479, 6:0.9115, 7:0.9128, 8:0.8690, 9:0.9142})\n",
    "\n",
    "#arr_id_threshold.update({0:0.94, 1:0.96, 2:0.94, 3:0.93, 4:0.96, 5:0.95, 6:0.94, 7:0.96, 8:0.96, 9:0.96})\n",
    "\n",
    "def run_new_SHINE(monitor_shine, X, pred, incoming_feature, threshold_shine):\n",
    "    features = features_train\n",
    "    #all labels from class c\n",
    "    ind_y_c = np.where(y_train == pred)[0]\n",
    "    #print('len all labels class',c, np.shape(ind_y_c), ind_y_c)\n",
    "\n",
    "    #all pred as c\n",
    "    ind_ML_c = np.where(pred_train==pred)[0]\n",
    "    #print('len pred class',c, np.shape(ind_ML_c), ind_ML_c)\n",
    "\n",
    "    #features from correct pred\n",
    "    ind = set(ind_y_c).intersection(ind_ML_c)\n",
    "    f_c_correct = features[list(ind)]\n",
    "\n",
    "    scores = []\n",
    "    for c in f_c_correct:\n",
    "        cosine_similarity = 1 - spatial.distance.cosine(c, incoming_feature)\n",
    "        scores.append(cosine_similarity)\n",
    "        \n",
    "    avg_sim = np.sum(scores)/len(scores)\n",
    "    \n",
    "    if avg_sim >= arr_id_threshold[pred]:\n",
    "        return False #it is ID and correct pred\n",
    "    elif avg_sim < arr_ood_threshold[pred]:\n",
    "        return True #it is OOD\n",
    "    else:\n",
    "        #it is not OOD but we do not know if the prediction is correct\n",
    "        monitor_pred, pdf = monitor_shine.predict(X, pred, threshold_shine)\n",
    "        \n",
    "        #temporary\n",
    "        if monitor_pred==True:\n",
    "            monitor_pred=False\n",
    "        elif monitor_pred==False:\n",
    "            monitor_pred==True\n",
    "        return monitor_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f36fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## experimenting new thresholds for shine\n",
    "def run_SHINE_2(monitor_shine, X, pred, incoming_feature, threshold_SMimg, threshold_SMout):\n",
    "    features = features_train[id_layer_monitored]\n",
    "    #all labels from class c\n",
    "    ind_y_c = np.where(id_y_train == pred)[0]\n",
    "    #print('len all labels class',c, np.shape(ind_y_c), ind_y_c)\n",
    "\n",
    "    #all pred as c\n",
    "    ind_ML_c = np.where(pred_train==pred)[0]\n",
    "    #print('len pred class',c, np.shape(ind_ML_c), ind_ML_c)\n",
    "\n",
    "    #features from correct pred\n",
    "    ind = set(ind_y_c).intersection(ind_ML_c)\n",
    "    f_c_correct = features[list(ind)]\n",
    "\n",
    "    scores = []\n",
    "    for c in f_c_correct:\n",
    "        cosine_similarity = 1 - spatial.distance.cosine(c, incoming_feature)\n",
    "        scores.append(cosine_similarity)\n",
    "        \n",
    "    len_scores = len(scores)\n",
    "    sorted_scores = sorted(scores)\n",
    "    ind_threshold = int(len_scores*threshold_SMout)\n",
    "    min_threshold_sim = sorted_scores[-ind_threshold]\n",
    "    max_threshold_sim = sorted_scores[ind_threshold]\n",
    "    #avg_sim = np.sum(scores)/len(scores)\n",
    "    \n",
    "    if max_threshold_sim >= arr_id_threshold[pred]:\n",
    "        return False #it is ID and correct pred\n",
    "    elif min_threshold_sim <= arr_ood_threshold[pred]:\n",
    "        return True #it is OOD\n",
    "    else:\n",
    "        #it is not OOD but we do not know if the prediction is correct\n",
    "        monitor_pred, pdf = monitor_shine.predict(X, pred, threshold_SMimg)\n",
    "        return monitor_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building oob monitor\n",
    "monitor_oob = OutsideTheBoxMonitor(n_clusters=3)\n",
    "monitor_oob.fit(features_train[id_layer_monitored], lab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ad23c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [20:07,  8.28it/s]\n"
     ]
    }
   ],
   "source": [
    "#testing SHINE  and OOB monitors monitors on ID testset\n",
    "m_true = []\n",
    "m_shine = []\n",
    "m_shine_2 = []\n",
    "m_oob = []\n",
    "\n",
    "threshold_SMimg = 0.1\n",
    "threshold_SMout = 0.9\n",
    "\n",
    "for x, pred, feature, label in tqdm(zip(x_test, pred_test, \n",
    "                                        features_test, y_test.flatten())):\n",
    "    #SHINE\n",
    "    monitor_shine_pred = run_new_SHINE(monitor_shine, np.array([x]), pred, feature, threshold_id)\n",
    "    m_shine.append(monitor_shine_pred)\n",
    "    \n",
    "    # SHINE relaxed thresholds\n",
    "    #monitor_shine_pred_2 = run_SHINE_2(monitor_shine2, np.array([x]), pred, feature,\n",
    "    #                                   threshold_SMimg, threshold_SMout)\n",
    "    #m_shine_2.append(monitor_shine_pred_2)\n",
    "    \n",
    "    #oob\n",
    "    #out_box = monitor_oob.predict([feature], [pred])\n",
    "    #m_oob.append(out_box)\n",
    "    \n",
    "    if pred == label: #monitor does not need to activate\n",
    "        m_true.append(0)\n",
    "    else: #monitor should activate\n",
    "        m_true.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2e6be7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [19:50,  8.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for x, feature, pred in tqdm(zip(ood_X_test, features_ood, pred_ood)):\n",
    "    #SHINE\n",
    "    monitor_shine_pred = run_new_SHINE(monitor_shine, np.array([x]), pred, feature, threshold_id)\n",
    "    m_shine.append(monitor_shine_pred)\n",
    "    \n",
    "    # SHINE relaxed thresholds\n",
    "    #monitor_shine_pred_2 = run_SHINE_2(monitor_shine, np.array([x]), pred, feature,\n",
    "    #                                   threshold_SMimg, threshold_SMout)\n",
    "    #m_shine_2.append(monitor_shine_pred_2)\n",
    "    \n",
    "    #oob\n",
    "    #out_box = monitor_oob.predict([feature], [pred])\n",
    "    #m_oob.append(out_box)\n",
    "    \n",
    "    m_true.append(1) #monitor should always react to novel classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1785ac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SHINE new\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.61      0.88      0.72      8971\n",
      "        True       0.85      0.54      0.66     11029\n",
      "\n",
      "    accuracy                           0.69     20000\n",
      "   macro avg       0.73      0.71      0.69     20000\n",
      "weighted avg       0.74      0.69      0.69     20000\n",
      "\n",
      "0.4401405353853451\n",
      "0.7110052248326706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef as mcc\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "m_true = np.array(m_true).astype(bool)\n",
    "m_shine = np.array(m_shine).astype(bool)\n",
    "#m_shine_2 = np.array(m_shine_2).astype(bool)\n",
    "#m_oob = np.array(m_oob).flatten().astype(bool)\n",
    "\n",
    "#evaluating new SHINE \n",
    "print('\\nSHINE new')\n",
    "print(classification_report(m_true, m_shine))\n",
    "print(mcc(m_true, m_shine))\n",
    "print(balanced_accuracy_score(m_true, m_shine))\n",
    "\n",
    "#evaluating new SHINE 2\n",
    "#print('\\nSHINE new 2')\n",
    "#print(classification_report(m_true, m_shine_2))\n",
    "#print(mcc(m_true, m_shine_2))\n",
    "#print(balanced_accuracy_score(m_true, m_shine_2))\n",
    "\n",
    "#evaluating OOB\n",
    "#print('\\nOOB')\n",
    "#print(classification_report(m_true, m_oob))\n",
    "#print(mcc(m_true, m_oob))\n",
    "#print(balanced_accuracy_score(m_true, m_oob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SHINE new\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       False       0.63      0.69      0.66      8971\n",
    "        True       0.73      0.67      0.69     11029\n",
    "\n",
    "    accuracy                           0.68     20000\n",
    "   macro avg       0.68      0.68      0.68     20000\n",
    "weighted avg       0.68      0.68      0.68     20000\n",
    "\n",
    "0.35391006402894293\n",
    "0.6778852519809273\n",
    "'''\n",
    "'''\n",
    "SHINE reverso\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "       False       0.61      0.88      0.72      8971\n",
    "        True       0.85      0.54      0.66     11029\n",
    "\n",
    "    accuracy                           0.69     20000\n",
    "   macro avg       0.73      0.71      0.69     20000\n",
    "weighted avg       0.74      0.69      0.69     20000\n",
    "\n",
    "0.4401405353853451\n",
    "0.7110052248326706\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
